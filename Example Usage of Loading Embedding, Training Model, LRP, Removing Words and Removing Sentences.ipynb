{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how each module should be used. Each method is documented and explained. Be careful that all Models, Embeddings and Data should included or it may crash or give errors.<br>\n",
    "\n",
    "Note that you do not need this notebook to use modules. This notebook is just for displaying the usage of them. To make it easier to understand.<br>\n",
    "<br><br>\n",
    "Index of this notebook:<br>\n",
    "1- Hyperparameters and initial setup <br>\n",
    "2- Loading embedding model<br>\n",
    "3- Loading preprocessed data<br>\n",
    "4- Training model (you can restore pre-trained model as well)<br>\n",
    "5- Restoring pre-trained model<br>\n",
    "6- Preparing system for for LRP<br>\n",
    "7- LRP Scores for the example in 6.<br>\n",
    "8- Removing important words according to TF-IDF (method 1)<br>\n",
    "9- Removing words uniquely (method 2)<br>\n",
    "10- Removing Sentences<br>\n",
    "11- Offering keywords to users (Method 1, better and more personal recommendations)<br>\n",
    "12- Offering keywords to users (Method 2, static method)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\P\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dermatology': 'Deri ve Zührevi Hastalıkları (Cildiye)', 'Internal Medicine': 'İç Hastalıkları (Dahiliye)', 'Neurology': 'Nöroloji', 'Obstetrics & Gynecology': 'Kadın Hastalıkları ve Doğum', 'Ophthalmology': 'Göz Hastalıkları', 'Orthopaedic Surgery': 'Ortopedi ve Travmatoloji', 'Otolaryngology': 'Kulak Burun Boğaz Hastalıkları', 'Pediatrics': 'Çocuk Sağlığı ve Hastalıkları', 'Psychiatry': 'Ruh Sağlığı ve Hastalıkları', 'Radiology-Diagnostic': 'Radyoloji', 'Surgery-General': 'Genel Cerrahi', 'Urology': 'Üroloji'}\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import lrp\n",
    "import EmbedHelper\n",
    "import DataLoader\n",
    "import Models\n",
    "import remover\n",
    "import utility\n",
    "import keyword_recommender\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Hyperparameters and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Fast Text', 2: 'Google News', 3: 'HealthTap', 4: 'Pubmed', 5: 'Glove', 6: 'iCliniq Trigram', 7: 'iCliniq default'}\n"
     ]
    }
   ],
   "source": [
    "embedDict = EmbedHelper.EmbeddingHandler.embedDict\n",
    "print(embedDict)\n",
    "configs = {\n",
    "    \"vectorSize\":300,\n",
    "    \"trainNewModel\":True,\n",
    "    \"dataColumn\":\"question\",\n",
    "    \"maxLength\":128,\n",
    "    \"batchSize\":8,\n",
    "    \"embeddingType\":embedDict[2],\n",
    "    \"ELMo\":True,\n",
    "    \"PreEmbed\":True,\n",
    "    \"restore\":True\n",
    "}\n",
    "\n",
    "inputSize = configs[\"maxLength\"]\n",
    "vectorSize = configs[\"vectorSize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Loading embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Google News\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "EmbedModel = EmbedHelper.EmbeddingHandler(configs[\"embeddingType\"], False, 300, \"Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iCliniq Data\n",
    "trainData = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_train_questions.npy\")\n",
    "trainTarget = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_train_target.npy\")\n",
    "testData = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_test_questions.npy\")\n",
    "testTarget = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_test_target.npy\")\n",
    "\n",
    "trainData_raw = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_train_questions_raw.npy\")\n",
    "testData_raw = np.load(\"data//icliniq//iCliniq_14K//icliniq_14k_test_questions_raw.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Training model (you can restore pre-trained model as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fullvectorsize:  300\n",
      "(?, 126, 1, 250)\n",
      "WARNING:tensorflow:From C:\\Users\\aaaaaa\\Jupyter Notebook\\Proje NLP Turk Telekom\\Documented Code\\Models.py:169: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Model Created.\n",
      "trainData shape :  (7903, 128)\n",
      "testData shape :  (1975, 128)\n",
      "trainTarget shape :  (7903,)\n",
      "testTarget shape :  (1975,)\n",
      "\n",
      "[Current iteration = 20 Train Acc:0.5 HT Test Acc:0 fold0Test: (0) ucAcc :0 dataRatio  :0 ]\r"
     ]
    }
   ],
   "source": [
    "# Test just for 32 iterations (for example purposes).\n",
    "sess, nnModel = helper.execute_training(False, EmbedModel, 32, trainData, trainTarget, testData, testTarget, configs, model_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Restoring pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fullvectorsize:  300\n",
      "(?, 126, 1, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\P\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from NNModels/icliniq14k_GoogleNews_onelayer_pad128/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "model_path = \"NNModels/icliniq14k_GoogleNews_onelayer_pad128/model.ckpt\"\n",
    "outputSize = 12\n",
    "nnModel = Models.CNN(inputSize=inputSize, vectorSize=vectorSize, outputSize=outputSize)\n",
    "\n",
    "sess = tf.InteractiveSession(graph=nnModel.paperGraph)\n",
    "tf.global_variables_initializer().run()\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "tf.train.Saver().restore(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Preparing system for for LRP\n",
    "\n",
    "We need a session and a NNModel to use LRP.I used the ones came after training model for example purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test LRP\n",
    "ClassDict = {}\n",
    "with open('fold0classDict.pkl', 'rb') as f:\n",
    "    ClassDict = pickle.load(f)\n",
    "outputSize = len(ClassDict)\n",
    "\n",
    "batch_x = trainData[0:21]\n",
    "batch_y = trainTarget[0:21]\n",
    "batch_x = EmbedModel.vectorizeBatch(batch_x)\n",
    "batch_y = sess.run(tf.one_hot(batch_y,outputSize)) \n",
    "\n",
    "alpha = 1\n",
    "layer_count = 1\n",
    "\n",
    "# Executing LRP\n",
    "weights, biases, activations = helper.get_weights_biases_acts(layer_count, nnModel)\n",
    "backprop_layers = lrp.lrp_layers(alpha, layer_count, activations, weights, biases)\n",
    "word_relevances, results_combined = lrp.get_word_relevances(alpha, backprop_layers, layer_count, batch_x[0:1], trainData[0], sess, nnModel, activations, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello', 'doctor', 'i', 'have', 'burning', 'sensation', 'while',\n",
       "       'urinating', 'and', 'a', 'frequent', 'urge', 'to', 'urinate',\n",
       "       'can', 'it', 'be', 'due', 'to', 'sexual', 'contact', 'i', 'am',\n",
       "       'a', 'year', 'old', 'male', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]', '[None]', '[None]', '[None]', '[None]', '[None]',\n",
       "       '[None]'], dtype='<U26')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- LRP Scores for each word (for example above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', -0.00826831935576656),\n",
       " ('doctor', -0.1080422055646184),\n",
       " ('i', 0.020342834972284517),\n",
       " ('have', -0.3017208268602165),\n",
       " ('burning', 0.44691333996067906),\n",
       " ('sensation', -0.0056952146834523204),\n",
       " ('while', -0.006010369613295131),\n",
       " ('urinating', -0.016021687008595818),\n",
       " ('and', 0.0),\n",
       " ('a', 0.0),\n",
       " ('frequent', 0.1610667028217684),\n",
       " ('urge', -0.012891035401162513),\n",
       " ('to', 0.0),\n",
       " ('urinate', -0.7028770827343872),\n",
       " ('can', 0.04179418528419647),\n",
       " ('it', 0.09591374834138497),\n",
       " ('be', 0.24346324413630724),\n",
       " ('due', -0.1294584272151936),\n",
       " ('to', 0.0),\n",
       " ('sexual', -0.08989809793069395),\n",
       " ('contact', -0.27382501712822876),\n",
       " ('i', 0.02026339918725208),\n",
       " ('am', -0.007761515742923862),\n",
       " ('a', 0.0),\n",
       " ('year', 0.001163108813086248),\n",
       " ('old', -0.002362008131448808),\n",
       " ('male', -0.0780253962949429),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0),\n",
       " ('[None]', 0.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_relevances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Removing important words according to TF-IDF (method 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method remove all occurrences of important words. For example, if remove_top_x_words is 1, then this method finds all occurrences of most important 1 word, lets assume the word is \"skin\", and removes all occurrences of \"skin\" from the data instance. So it may remove more than one words even if remove_top_x_words is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_imps_all_classes = remover.get_word_imps_all_classes(\"data//icliniq//iCliniq_14K//tfidf_results//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_words_removed 1063\n"
     ]
    }
   ],
   "source": [
    "# Remove most important 2 words\n",
    "confidence = 0.1 # not important here\n",
    "remove_top_x_words = 2\n",
    "removed_data = remover.remove_x_tfidf(remove_top_x_words, testData, testTarget, word_imps_all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7053164556962025"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = helper.evaluatePerformance(configs, nnModel, EmbedModel, sess, removed_data, testTarget, 1, 1-confidence)\n",
    "results[\"Accuracy\"] # accuracy after removing important words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9- Removing words uniquely (method 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to previous method, this method only removes specified number of words from data. For example, if the most important 2 words are \"skin\" and \"hair\", then this method removes only \"skin\" and \"hair\" in the specified index, and does not remove other \"skin\" or \"hair\" words from the data instance. This is because even same words in different indexes may have different importance values.\n",
    "\n",
    "You can either restore previously saved list, or create a new one (takes some time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore previously saved word_imps\n",
    "word_imps_unsorted = np.load(\"data//icliniq//iCliniq_14K//word_importances_unsorted.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or generate new, which takes a while\n",
    "should_save = True\n",
    "save_path = \"data//icliniq//iCliniq_14K//word_importances_unsorted\"\n",
    "word_imps_unsorted = remover.get_word_importances(testData, EmbedModel, nnModel, sess, should_save, save_path)\n",
    "np.save(\"data//icliniq//iCliniq_14K//word_importances_unsorted\", word_imps_unsorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want unsorted word importances list because we need indexes according to data. We sort after we add indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7048101265822785"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_imps_with_indexes = remover.prepare_word_imps(word_imps_unsorted, testData)\n",
    "removed_data = remover.remove_words_with_index(5, testData, word_imps_with_indexes)\n",
    "removed_data = np.array(removed_data)\n",
    "\n",
    "results = helper.evaluatePerformance(configs, nnModel, EmbedModel, sess, removed_data, testTarget, 1, 1-confidence)\n",
    "results[\"Accuracy\"] # accuracy after removing important \"exacty\" 5 words from each data instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- Removing Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like method 1 of removing words, this method may remove more than one sentences for each important word. For example if remove_top_x_words is 1, and lets assume our most important word is \"hair\", then this method removes all sentences which includes the word \"hair\" in current data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important words for classes according to tfidf \n",
    "word_imps_all_classes = remover.get_word_imps_all_classes(\"data//icliniq//iCliniq_14K//tfidf_results//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100.00 % %\n",
      "deleted_sentence_count 648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7154430379746836"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_top_x_words = 2\n",
    "\n",
    "sentences_removed_data, indexes = remover.remove_important_sentences(remove_top_x_words, False, testData_raw, testTarget, word_imps_all_classes, 128)\n",
    "# sentence_removed_data results all instances\n",
    "results = helper.evaluatePerformance(configs, nnModel, EmbedModel, sess, np.array(sentences_removed_data), testTarget, 1, 1-confidence)\n",
    "results[\"Accuracy\"] # accuracy after removing sentences according to top 2 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11- Offering keywords to users (Method 1, better and more personal recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = np.load(\"data//symcat//symcat_data.npy\")\n",
    "symptoms = np.load(\"data//symcat//symcat_categories.npy\")\n",
    "desc_plus_symptoms = np.load(\"data//symcat//symcat_data_plus_cats.npy\")\n",
    "softmax_results_symcat = np.load(\"data//symcat//softmax_results_symcat.npy\")\n",
    "\n",
    "\n",
    "# contains similarities for all classes with all symptoms\n",
    "# for example, cosine_sim_indexes_all_classes[0] is Dermatology (ClassDict)\n",
    "# cosine_sim_indexes_all_classes[0][0] is the index most relevant symptom in symptoms array\n",
    "cosine_sim_indexes_all_classes = np.load(\"data//symcat//cosine_sim_indexes_all_classes.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_user_input = \"hello doctor i have been struggling with acne for a while now and have been postponing going to my general physician i have been very hesitant on what to use and do not ever apply anything apart from a light cleanser on my face i am not even sure what the term would be for it except the loose term rosacea i am looking for advice on what prescription or non prescription medication would be safe and effective for my acne with concerns of my nose getting worse i also have very pale skin\" # example input\n",
    "confidence = 0.9\n",
    "user_input = DataLoader.DataHandler.cleanTextData(raw_user_input)\n",
    "user_input = np.array(DataLoader.DataHandler.textIntoWordList(user_input, 128)[0])\n",
    "#user input lrpye verilmeye hazır\n",
    "# user_input = process_user_input([raw_user_input])\n",
    "all_results = helper.evaluatePerformance(configs, nnModel, EmbedModel, sess, user_input, [0], 1, 1-confidence)\n",
    "\n",
    "user_result = all_results[\"Scores\"][0] # we will check similarity between user_result and softmax_results_symcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloudy eye'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get most relevant symptom and ask this to user\n",
    "keyword_recommender.get_symptom_input_similarity(0, symptoms, softmax_results_symcat, all_results, raw_user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12- Offering keywords to users (Method 2, static method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " # example input\n",
    "raw_user_input = \"my hair is transparent\"\n",
    "\n",
    "user_input = keyword_recommender.process_user_input([raw_user_input])\n",
    "all_results = helper.evaluatePerformance(configs, nnModel, EmbedModel, sess, user_input, [0], 1, 1-confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal:  blood in urine\n",
      "personal:  vulvar symptoms\n",
      "personal:  lump or mass of breast\n",
      "static:  skin irritation\n",
      "static:  mouth symptoms\n",
      "static:  problems with lymph nodes (glands)\n",
      "static:  sore lymph nodes\n",
      "static:  symptoms of eye\n",
      "static:  visual disturbance\n"
     ]
    }
   ],
   "source": [
    "print(\"personal: \", keyword_recommender.get_symptom_input_similarity(0, symptoms, softmax_results_symcat, all_results, raw_user_input))\n",
    "print(\"personal: \", keyword_recommender.get_symptom_input_similarity(1, symptoms, softmax_results_symcat,all_results, raw_user_input))\n",
    "print(\"personal: \", keyword_recommender.get_symptom_input_similarity(2, symptoms, softmax_results_symcat, all_results, raw_user_input))\n",
    "print(\"static: \", keyword_recommender.get_next_symptom_top3class(0, 0, symptoms, cosine_sim_indexes_all_classes, all_results, raw_user_input))\n",
    "print(\"static: \", keyword_recommender.get_next_symptom_top3class(1, 0, symptoms, cosine_sim_indexes_all_classes, all_results, raw_user_input))\n",
    "print(\"static: \", keyword_recommender.get_next_symptom_top3class(0, 1, symptoms, cosine_sim_indexes_all_classes, all_results, raw_user_input))\n",
    "print(\"static: \", keyword_recommender.get_next_symptom_top3class(1, 1, symptoms, cosine_sim_indexes_all_classes, all_results, raw_user_input))\n",
    "print(\"static: \", keyword_recommender.get_next_symptom_top3class(0, 2, symptoms, cosine_sim_indexes_all_classes, all_results, raw_user_input))\n",
    "print(\"static: \", keyword_recommender.get_next_symptom_top3class(1, 2, symptoms, cosine_sim_indexes_all_classes, all_results, raw_user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
